{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmarking on One Machine \n",
    "=========\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Typical User Experience**\n",
    "\n",
    "Laptop Specs:\n",
    "    \n",
    "    Intel Core i7\n",
    "    16gb RAM\n",
    "    NVIDIA GeForce GTX 965M 2GB GDDR5 memory \n",
    "    Microsoft Windows 10\n",
    "    Running Jupyter Notebooks and multiple programs in the background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "  function code_toggle() {\n",
       "    if (code_shown){\n",
       "      $('div.input').hide('500');\n",
       "      $('#toggleButton').val('Show Code')\n",
       "    } else {\n",
       "      $('div.input').show('500');\n",
       "      $('#toggleButton').val('Hide Code')\n",
       "    }\n",
       "    code_shown = !code_shown\n",
       "  }\n",
       "\n",
       "  $( document ).ready(function(){\n",
       "    code_shown=false;\n",
       "    $('div.input').hide()\n",
       "  });\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('''<script>\n",
    "  function code_toggle() {\n",
    "    if (code_shown){\n",
    "      $('div.input').hide('500');\n",
    "      $('#toggleButton').val('Show Code')\n",
    "    } else {\n",
    "      $('div.input').show('500');\n",
    "      $('#toggleButton').val('Hide Code')\n",
    "    }\n",
    "    code_shown = !code_shown\n",
    "  }\n",
    "\n",
    "  $( document ).ready(function(){\n",
    "    code_shown=false;\n",
    "    $('div.input').hide()\n",
    "  });\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"></form>''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "import time #cpu time\n",
    "import psutil #memory usage\n",
    "#tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "#Scikitlearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from scipy.sparse import coo_matrix,csr_matrix,lil_matrix\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# For Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "#displays better in jupyter notebooks\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://repo.anaconda.com/archive/Anaconda2-5.3.0-Linux-x86_64.sh\n",
    "#export PATH=\"/home/ubuntu/anaconda2/bin:$PATH\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install numpy \n",
    "#conda install -c intel mkl\n",
    "#!pip install matplotlib --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For benchmarking to start with we are using LIBSVM's **Avazu-App data** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Source(s)**: \n",
    "\n",
    "LIBSVM Data Classification: https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#a1a\n",
    "\n",
    "Avazu's Click-through Prediction https://www.kaggle.com/c/avazu-ctr-prediction/data\n",
    "\n",
    "**Preprocessing**: This data is used in a competition on click-through rate prediction jointly hosted by Avazu and Kaggle in 2014. The participents were asked to learn a model from the first 10 days of advertising log, and predict the click probability for the impressions on the 11th day. The data sets here are generated by applying our winning solution without some complicated components. To reproduce this data, you can execute our code and see the results in the directory \"base.\" For better test scores, we divide the data to two disjoint groups \"app\" and \"site,\" and conduct training and prediction tasks on the two groups independetly. Specifically, each instance has either \"site_id=85f751fd\" or \"app_id=ecad2386,\" and these two feature values never co-occur. Thus we can split the data set according to them. The organizers do not disclose the test labels, so the labels in the test sets are not meaningful. To obtain a test score, please use the code provided below to generate and submit a file to the competition site. Because data are timely dependent, cross validation is not suitable for parameter selection. We provide a training-validation split (e.g., \"avazu-app.tr\" and \"avazu-app.val\") by consider the last 4,218,938 training instances for validation. [YJ16a]\n",
    "\n",
    "- Number of classes: 2\n",
    "\n",
    "- Number of data: 40,428,967 / 4,577,464 (testing) / 14,596,137 (avazu-app) / 1,719,304 (avazu-app.t) / 12,642,186 (avazu-app.tr) / 1,953,951 (avazu-app.val) / 25,832,830 (avazu-site) / 2,858,160 (avazu-site.t) / 23,567,843 (avazu-site.tr) / 2,264,987 (avazu-site.val)\n",
    "\n",
    "    Number of features: 1,000,000\n",
    "Files:\n",
    "\n",
    "- avazu-app.bz2 (app) https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/avazu-app.bz2\n",
    "- avazu-app.t.bz2 (app's testing) https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/avazu-app.t.bz2\n",
    "- avazu-app.tr.bz2 (app's tr) https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/avazu-app.tr.bz2\n",
    "- avazu-app.val.bz2 (app's val) https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/avazu-app.val.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download data \n",
    "#!wget https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/a1a.t # <--data just to make sure things work\n",
    "#!wget https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/avazu-app.bz2 #<---full app data\n",
    "#!wget https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/avazu-app.tr.bz2 #<---benchmark training\n",
    "#!wget https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/avazu-app.t.bz2 #<---benchmark testing\n",
    "#!wget https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/avazu-app.val.bz2 #<---benchmark validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting Files:\n",
    "    \n",
    "    For Scikitlearn extracting the files is unnecessary since the load_svm_light_file with automatically extract .bz2 files. However for Tensorflow will need them already extracted to have comparable results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!bzip2 -dk avazu-app.tr.bz2 #Extract Training\n",
    "#!bzip2 -dk avazu-app.t.bz2 #Extract Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Classes and Functions***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data importing function\n",
    "def get_data(file):\n",
    "    data = load_svmlight_file(file)#avazu-app.tr.bz2\n",
    "    return data[0], data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For Importing Raw Individual File**\n",
    "\n",
    "*If the data is in a single file that hasn't already been seperated in training and testing datasets.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Here is the memory baseline prior to importing data:\\n', svmem(total=8369991680, available=6987788288, percent=16.5, used=1092698112, free=4756865024, active=2981306368, inactive=455618560, buffers=113709056, cached=2406719488, shared=21860352, slab=108249088))\n"
     ]
    }
   ],
   "source": [
    "mem_baseline=psutil.virtual_memory() #  physical memory usage\n",
    "print('Here is the memory baseline prior to importing data:\\n',mem_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_location=os.getcwd()+'/avazu-app.tr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y =get_data(file_location) #import raw data\n",
    "mem_InData=psutil.virtual_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Here is the memory usage after importing data:\\n', svmem(total=8369991680, available=4503855104, percent=46.2, used=3531640832, free=725569536, active=5411880960, inactive=1915072512, buffers=144220160, cached=3968561152, shared=21860352, slab=246988800))\n",
      "\n",
      "The time taken to import the raw data:\n"
     ]
    }
   ],
   "source": [
    "print('Here is the memory usage after importing data:\\n',mem_InData) #  physical memory usage\n",
    "print('\\nThe time taken to import the raw data:')\n",
    "exec_time1 = %%timeit -o X, y =get_data(file_location) #import raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Here is the type of sparse matrix format for the data:')\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "#May not be appropriate for time dependant data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** For Importing Previously Seperated Training and Testing File**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data is already seperated into a training and testing file, you should begin your testing here.              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Designate file location\n",
    "train_file_location=os.getcwd()+'/avazu-app.tr'\n",
    "test_file_location=os.getcwd()+'/avazu-app.t'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Here is the memory baseline prior to importing data:\\n\\n', svmem(total=8369991680, available=7271579648, percent=13.1, used=831164416, free=7229689856, active=813244416, inactive=201641984, buffers=28139520, cached=280997888, shared=21852160, slab=62763008))\n"
     ]
    }
   ],
   "source": [
    "mem_baseline=psutil.virtual_memory() #  physical memory usage\n",
    "print('Here is the memory baseline prior to importing data:\\n\\n',mem_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train=get_data(train_file_location)\n",
    "X_test, y_test=get_data(test_file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Here is the memory usage afer importing the data:\\n\\n', svmem(total=8369991680, available=4499521536, percent=46.2, used=3603218432, free=1644941312, active=3578728448, inactive=3014131712, buffers=28303360, cached=3093528576, shared=21852160, slab=62894080))\n"
     ]
    }
   ],
   "source": [
    "mem_baseline=psutil.virtual_memory() #  physical memory usage\n",
    "print('Here is the memory usage afer importing the data:\\n\\n',mem_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute Logistic Regression in SciKit Learn\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters for the logistic regression (Gradient Descent) are:\n",
    "    \n",
    "    Regularization: L2\n",
    "    \n",
    "    Regularization Threshold (C): 1.0\n",
    "    \n",
    "    Tolerance: 0.001\n",
    "    \n",
    "    Fit Intercpt: Yes (True)\n",
    "    \n",
    "    Processors (n_jobs): 1\n",
    "    \n",
    "    Max Number of Iterations: 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a logistic regression model, and fit with X and y\n",
    "\n",
    "model = linear_model.LogisticRegression(penalty='l2',\\\n",
    "                                        C=1.0,\\\n",
    "                                        tol=0.001,\\\n",
    "                                        fit_intercept=True,\\\n",
    "                                        n_jobs=1,\\\n",
    "                                        max_iter=100)\n",
    "\n",
    "model = model.fit(X_train, y_train.ravel())#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391 ms ± 28.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "\n",
      "The average time taken to execute the logistic regression: 391 ms ± 28.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) seconds\n"
     ]
    }
   ],
   "source": [
    "exec_time2 = %%timeit -o model.fit(X_train, y_train.ravel())\n",
    "print('\\nThe average time taken to execute the logistic regression:',exec_time2,'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup variables for evaluation metrics\n",
    "y_pred = model.predict(X_test)\n",
    "y_obs = y_test\n",
    "y_score = y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The correlation coefficient: 0.1846532267117451 \n",
      "The accuracy of the model: 0.8495910927184944 \n",
      "The precision (tp / (tp + fp)): 0.7361835245046924 \n",
      "The recall (tp / (tp + fn)): 0.5912897822445561 \n",
      "The f1 score is: 0.655829075708314 \n",
      "The Area Under the Curve score is: 0.76239916181873\n"
     ]
    }
   ],
   "source": [
    "r2=metrics.r2_score(y_obs, y_pred)\n",
    "accuracy=model.score(X_train, y_train)\n",
    "prec=metrics.precision_score(y_obs, y_pred, labels=None, pos_label=1)\n",
    "recall=metrics.recall_score(y_obs, y_pred, labels=None, pos_label=1)\n",
    "f1 = metrics.f1_score(y_obs,y_pred)\n",
    "ROC_AUC = metrics.roc_auc_score(y_obs, y_score)\n",
    "print('The correlation coefficient:',r2,\\\n",
    "      '\\nThe accuracy of the model:',accuracy,\\\n",
    "      '\\nThe precision (tp / (tp + fp)):',prec,\\\n",
    "      '\\nThe recall (tp / (tp + fn)):',recall,\\\n",
    "      '\\nThe f1 score is:',f1,\\\n",
    "      '\\nThe Area Under the Curve score is:',ROC_AUC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute Logistic Regression in Tensorflow\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the GPU is used for the task but restricted to using only 75% of GPU resources in order to prevent crashing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate =  0.001\n",
    "max_iter = 100\n",
    "batch_size = 30\n",
    "\n",
    "train_file = os.getcwd()+'\\\\tensorflow-models\\\\data\\\\libsvm_data\\\\a1a'\n",
    "test_file = os.getcwd()+'\\\\tensorflow-models\\\\data\\\\libsvm_data\\\\a1a.t'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Classes and Functions***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(object):\n",
    "    def __init__(self):\n",
    "        self.iter = 0\n",
    "        self.epoch_pass = 0\n",
    "\n",
    "    def load(self, file):\n",
    "        '''\n",
    "        '''\n",
    "        self.ins_num = 0 #<set at zero\n",
    "        f = open(file, \"r\")\n",
    "        self.y = []\n",
    "        self.feature_ids = []\n",
    "        self.feature_values = []\n",
    "        self.ins_feature_interval = []\n",
    "        self.max_ins_feature_interval = []\n",
    "        self.ins_feature_interval.append(0)#makes zero the starting value in ins_feature_interval\n",
    "        self.max_token=[]\n",
    "        for line in f.readlines():#iterating through open file\n",
    "            regexp = re.compile(r':')#<---If feature has a colon then do\n",
    "            tokens = line.split(\" \")#split lines in the file\n",
    "            #tokens.remove('\\n')\n",
    "            #print(tokens[0])\n",
    "            self.y.append(float(tokens[0]))#append to y the first value in tokens (+1,-1,1)\n",
    "            try:\n",
    "                tokens[-1] = tokens[-1].strip()#<----remove '\\n'\n",
    "                tokens.remove('') #<---remove '' empty in list\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            #                         last value in list is that value + (line splits -1)<--maybe adjusting for return (\\n) or y value \n",
    "            #print(self.ins_feature_interval[-1]+ len(tokens)-1)#<----stacks the batch sizes\n",
    "            self.ins_feature_interval.append(self.ins_feature_interval[-1]+ len(tokens)-1)\n",
    "            #print(len(self.ins_feature_interval))\n",
    "            for feature in tokens:#(len(tokens)~16\n",
    "              #  print(feature)\n",
    "                if regexp.search(feature):#if there is a colon in feature\n",
    "                    self.max_token.append(feature)#check on size\n",
    "                    feature_id, feature_value = feature.split(\":\") #split on colon\n",
    "                    if feature_id:\n",
    "                        self.feature_ids.append(int(feature_id))#append to feature ids\n",
    "                        self.feature_values.append(float(feature_value)) #append feature values\n",
    "            self.ins_num += 1 #set ins_num to 1\n",
    "        self.feature_num=max(self.feature_ids)#modify feature_num to max of ids (maximum # of features)\n",
    "        #self.max_ins_feature_interval=max(self.ins_feature_interval)\n",
    "        print('the max number of features:',self.feature_num)\n",
    "    \n",
    "\n",
    "    def mini_batch(self, batch_size):\n",
    "        begin = self.iter #begins as 0 as defined above\n",
    "        end = self.iter #starts with 0 as defined above\n",
    "        if self.iter + batch_size > self.ins_num: #if 0 + batchsize(10) > ins_num(1) defined in def load\n",
    "            end = self.ins_num #set end to be ins_num(1) \n",
    "            self.iter = 0 #set iter to 0\n",
    "            self.epoch_pass += 1 #add +1 to epoch_pass \n",
    "        else:\n",
    "            end += batch_size#add batch size to end, which should be equal to batch size\n",
    "            #print('end:',end)\n",
    "            #print('batch_size:',batch_size)\n",
    "            self.iter = end#set self.iter to batch size\n",
    "            #(begin, end)setting bounds moving across batch length in data\n",
    "            #print((begin, end))#slicing action of data (0, 15) (15, 30) (30, 45) (60,...\n",
    "        return self.slice(begin, end)\n",
    "#Error\n",
    "    def slice(self, begin, end):\n",
    "            sparse_index = []\n",
    "            sparse_ids = []\n",
    "            sparse_values = []\n",
    "            sparse_shape = []\n",
    "            max_feature_num = 0\n",
    "            for i in range(begin, end):#within range begin, end\n",
    "            #              15,461,906,1351                  0,446,891,1338 =~15 supposed to be length of token\n",
    "            #          (token length + range number +1) - (token length + range number)\n",
    "                feature_num = self.ins_feature_interval[i + 1] - self.ins_feature_interval[i]\n",
    "                if feature_num > max_feature_num:\n",
    "                    max_feature_num = feature_num\n",
    "                                    #       0,446,891,1338                  15,461,906,1351\n",
    "                #(token length + range number) ,       (token length + range number +1)\n",
    "                #self.max_ins_feature_interval-len(self.feature_ids)<-------------------\n",
    "                #print(self.ins_feature_interval[i],self.ins_feature_interval[i + 1])\n",
    "                #print(self.feature_ids[i])\n",
    "                for j in range(self.ins_feature_interval[i], self.ins_feature_interval[i + 1]):\n",
    "                #print(j,(len(self.feature_ids)))\n",
    "                #print(self.ins_feature_interval[i + 1])\n",
    "                #print(range(self.ins_feature_interval[i], self.ins_feature_interval[i + 1]))\n",
    "                #15 vals:                  0  , 0-14,446-460,891-905\n",
    "                    sparse_index.append([i - begin, j - self.ins_feature_interval[i]]) # index must be accent\n",
    "                    #[0, 0]-[0, 14][0, 0]-[0, 12]\n",
    "                    #print([i - begin, j - self.ins_feature_interval[i]])\n",
    "                    sparse_ids.append(self.feature_ids[j])\n",
    "                    sparse_values.append(self.feature_values[j])\n",
    "            sparse_shape.append(end - begin)\n",
    "            #print(end - begin)#<-----30\n",
    "            sparse_shape.append(max_feature_num)\n",
    "            #print(max_feature_num)#<-----15\n",
    "            #       Creates array shape of 30,1 of y values  (30, 1)\n",
    "            y = np.array(self.y[begin:end]).reshape((end - begin, 1))\n",
    "            #begin:0,30,60,90,120,150,180,210 intervals of 30\n",
    "            #end: 30,60,90,120,150,180,210,240\n",
    "            #            0            0                       0                 30            30\n",
    "            #print(len(sparse_index), len(sparse_ids), len(sparse_values), len(sparse_shape), len(y))\n",
    "            return (sparse_index, sparse_ids, sparse_values, sparse_shape, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticRegression(object):\n",
    "    def __init__(self, feature_num):\n",
    "        self.feature_num = feature_num\n",
    "        self.sparse_index = tf.placeholder(tf.int64)\n",
    "        self.sparse_ids = tf.placeholder(tf.int64)\n",
    "        self.sparse_values = tf.placeholder(tf.float32)\n",
    "        self.sparse_shape = tf.placeholder(tf.int64)\n",
    "        self.w = tf.Variable(tf.random_normal([self.feature_num, 1], stddev=0.1))\n",
    "        self.y = tf.placeholder(\"float\", [None, 1])\n",
    "\n",
    "    def forward(self):\n",
    "        return tf.nn.embedding_lookup_sparse(self.w,\n",
    "                                             tf.SparseTensor(self.sparse_index, self.sparse_ids, self.sparse_shape),\n",
    "                                             tf.SparseTensor(self.sparse_index, self.sparse_values, self.sparse_shape),\n",
    "                                             combiner=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the memory baseline prior to importing data:\n",
      "\n",
      " svmem(total=17101512704, available=10041020416, percent=41.3, used=7060492288, free=10041020416)\n"
     ]
    }
   ],
   "source": [
    "mem_baseline=psutil.virtual_memory() #  physical memory usage\n",
    "print('Here is the memory baseline prior to importing data:\\n\\n',mem_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the max number of features: 119\n",
      "the max number of features: 123\n",
      "\n",
      "The time taken to import and prepare data for Tensorflow is: 1.808777093887329\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "train_set = DataSet()\n",
    "train_set.load(train_file)\n",
    "test_set = DataSet()\n",
    "test_set.load(test_file)\n",
    "feature_num=test_set.feature_num\n",
    "\n",
    "end = time.time()\n",
    "exec_time=(end - start)\n",
    "print('\\nThe time taken to import and prepare data for Tensorflow is:',exec_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the memory usage after importing data:\n",
      " svmem(total=17101512704, available=9906634752, percent=42.1, used=7194877952, free=9906634752)\n"
     ]
    }
   ],
   "source": [
    "mem_baseline=psutil.virtual_memory() #  physical memory usage\n",
    "print('Here is the memory usage after importing data:\\n',mem_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BinaryLogisticRegression(feature_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=model.y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_output = tf.nn.sigmoid(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session = tf.Session()\n",
    "init_all_variable = tf.global_variables_initializer()\n",
    "init_local_variable = tf.local_variables_initializer()\n",
    "session.run([init_all_variable, init_local_variable])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set number of passes in for loop. \n",
    "\n",
    "*This is done primarily for timing purposes to get an average calculation time.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_passes=5 #number of passes in for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  100  ROC AUC score is:  0.5\n",
      "epoch:  100  ROC AUC score is:  0.5\n",
      "epoch:  100  ROC AUC score is:  0.5\n",
      "epoch:  100  ROC AUC score is:  0.5\n",
      "epoch:  100  ROC AUC score is:  0.5\n",
      "\n",
      "The average time taken to execute logistic regression for 5 full passes of 100 iterations took 0.00516057014465332 seconds with a standard deviation of +- 0.0028760574334623728\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "end_list=[]\n",
    "for i in range(0,num_passes):\n",
    "    while train_set.epoch_pass < max_iter:\n",
    "        sparse_index, sparse_ids, sparse_values, sparse_shape, mb_y = train_set.mini_batch(batch_size)\n",
    "        _, loss_, prob_out = session.run([optimizer, loss, probability_output],\n",
    "                                         feed_dict={model.sparse_index: sparse_index,\n",
    "                                                    model.sparse_ids: sparse_ids,\n",
    "                                                    model.sparse_values: sparse_values,\n",
    "                                                    model.sparse_shape: sparse_shape,\n",
    "                                                    model.y: mb_y})\n",
    "        \n",
    "    end = time.time()\n",
    "    exec_time=(end - start)\n",
    "    end_list.append(exec_time) \n",
    "\n",
    "    try:\n",
    "        auc = roc_auc_score(mb_y, prob_out)\n",
    "        print(\"epoch: \", train_set.epoch_pass, \" ROC AUC score is: \", auc)\n",
    "\n",
    "    except:\n",
    "        print('\\nValueError: Only one class present in y_true. ROC AUC score is not defined in that case.\\n')\n",
    "        print(mb_y.T)\n",
    "        print(prob_out.T,'\\n')\n",
    "\n",
    "print('\\nThe average time taken to execute logistic regression for '+str(num_passes)+' full passes of',max_iter,'iterations took',np.array(end_list).mean(),'seconds with a standard deviation of +- '+str(np.array(end_list).std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmarks\n",
    "======"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average time taken to import data:\n",
    "\n",
    "    SciKitLearn:\n",
    "    \n",
    "    Tensorflow:\n",
    "    \n",
    "    RocketML:\n",
    "\n",
    "Memory Usage:\n",
    "    \n",
    "    SciKitLearn Before importing: svmem(total=8369991680, available=6985629696, percent=16.5, used=1094762496, free=4751126528, active=2986393600, inactive=456204288, buffers=114388992, cached=2409713664, shared=21860352, slab=108478464)\n",
    "    \n",
    "    SciKitLearn After importing:svmem(total=8369991680, available=4224389120, percent=49.5, used=3855998976, free=1019949056, active=5932777472, inactive=1237405696, buffers=114442240, cached=3379601408, shared=21860352, slab=108474368))\n",
    "    \n",
    "Average Calculation Time:\n",
    "    \n",
    "    SciKitLearn:\n",
    "        \n",
    "    Tensorflow:\n",
    "    \n",
    "    RocketML:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
